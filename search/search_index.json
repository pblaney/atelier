{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Workshop for Genomic Data Science","text":""},{"location":"#scripts","title":"Scripts","text":"<p>A collection of scripts to help with anything from managing data to executing specific workflows.</p>"},{"location":"#reference","title":"Reference","text":"<p>Man pages for scripts</p>"},{"location":"#resources","title":"Resources","text":"<p>Files, websites, software, and papers I find useful.</p>"},{"location":"#recipes","title":"Recipes","text":"<p>Files to use as a starting point for scripts, READMEs, and other things</p>"},{"location":"resources/","title":"Resources","text":"<p>A collection of resources</p>"},{"location":"resources/#myeloma-genomics","title":"Myeloma Genomics","text":"<p>Coming soon ...</p>"},{"location":"resources/#templates","title":"Templates","text":"<p>Templates and recipes for how to use them</p>"},{"location":"resources/#aws","title":"AWS","text":"Recipe Description <code>user-collaborator</code> Create a secure new user account for an external collaborator"},{"location":"scripts/","title":"Scripts","text":"<p>A collection of scripts</p> Script Description <code>bamCleaner.sh</code> Tidy up BAMs with invalid MAPQs <code>fileRenamer.sh</code> Rename sets of files <code>md5Checker.sh</code> Generate/verify MD5 checksums <code>s3Mover.sh</code> Copy files between local and S3 locations <code>sraFastqExtractor.sh</code> Extract FASTQs from prefetched SRA data <code>sraPrefetcher.sh</code> Prefetch SRA data from NCBI"},{"location":"man/bamcleaner/","title":"Atelier","text":"","tags":["bam","hpc","batch-processing","slurm"]},{"location":"man/bamcleaner/#overview","title":"Overview","text":"<p>Cleans up BAM files by fixing MAPQ issues in unmapped reads and automatically validates BAM file integrity.</p>","tags":["bam","hpc","batch-processing","slurm"]},{"location":"man/bamcleaner/#usage","title":"Usage","text":"bamCleaner.sh<pre><code>sbatch --job-name=[jobName] ~/atelier/bin/bamCleaner.sh -i [input] [options]\n</code></pre>","tags":["bam","hpc","batch-processing","slurm"]},{"location":"man/bamcleaner/#options","title":"Options","text":"Option Description <code>-i [input]</code> Input BAM file or file list (one path per line) <code>-o [outputDir]</code> Output directory for cleaned BAMs (default: same as input) <code>-p [prefix]</code> Output filename prefix (default: original_name.cleaned) <code>-t [threads]</code> Number of threads to use (default: auto-detect) <code>-r</code> Remove unmapped reads entirely (default: keep with MAPQ=0) <code>-n</code> Dry run <code>-h</code> Display help message","tags":["bam","hpc","batch-processing","slurm"]},{"location":"man/bamcleaner/#examples","title":"Examples","text":"<p>Clean a single BAM (automatic validation always happens) <pre><code>sbatch --job-name=clean-bam ~/atelier/bin/bamCleaner.sh \\\n    -i sample.bam\n</code></pre></p> <p>Remove unmapped reads with automatic validation <pre><code>sbatch --job-name=clean-remove ~/atelier/bin/bamCleaner.sh \\\n    -i sample.bam \\\n    -r\n</code></pre></p> <p>Batch processing with automatic validation <pre><code>sbatch --job-name=clean-batch ~/atelier/bin/bamCleaner.sh \\\n    -i bam_list.txt\n</code></pre></p> <p>Dry run (shows what would happen, including validation) <pre><code>sbatch --job-name=clean-preview ~/atelier/bin/bamCleaner.sh \\\n    -i sample.bam \\\n    -n\n</code></pre></p>","tags":["bam","hpc","batch-processing","slurm"]},{"location":"man/filerenamer/","title":"Atelier","text":"","tags":["hpc"]},{"location":"man/filerenamer/#overview","title":"Overview","text":"<p>Rename large sets of files.</p>","tags":["hpc"]},{"location":"man/filerenamer/#usage","title":"Usage","text":"fileRenamer.sh<pre><code>~/atelier/bin/fileRenamer.sh -f [mappingFile] [options]\n</code></pre>","tags":["hpc"]},{"location":"man/filerenamer/#options","title":"Options","text":"Option Description <code>-f [mappingFile]</code> Text file with old and new filenames with format old_namenew_name (one path per line) <code>-d [sourceDir]</code> Source directory for files (default: current directory) <code>-n</code> Dry run <code>-v</code> Verbose output with debug logging <code>-h</code> Display help message","tags":["hpc"]},{"location":"man/filerenamer/#examples","title":"Examples","text":"<p>Basic rename <pre><code>~/atelier/bin/fileRenamer.sh -f rename_list.txt\n</code></pre></p> <p>Dry run preview <pre><code>~/atelier/bin/fileRenamer.sh -f rename_list.txt -n\n</code></pre></p> <p>Rename files in specific directory <pre><code>~/atelier/bin/fileRenamer.sh -f rename_list.txt -d /data/samples/\n</code></pre></p> <p>Verbose with dry run <pre><code>~/atelier/bin/fileRenamer.sh -f rename_list.txt -n -v\n</code></pre></p>","tags":["hpc"]},{"location":"man/md5checker/","title":"Atelier","text":"","tags":["hpc","batch-processing","slurm"]},{"location":"man/md5checker/#overview","title":"Overview","text":"<p>Calculates MD5 checksums for files or verifies files against existing checksums. It supports batch processing via file patterns, file lists, or directory recursion.</p>","tags":["hpc","batch-processing","slurm"]},{"location":"man/md5checker/#usage","title":"Usage","text":"md5Checker.sh<pre><code>sbatch --job-name=[jobName] ~/atelier/bin/md5Checker.sh [options] -o [output]\n</code></pre>","tags":["hpc","batch-processing","slurm"]},{"location":"man/md5checker/#options","title":"Options","text":"Option Description <code>-p [pattern]</code> File pattern/glob to match files <code>-f [fileList]</code> Text file containing list of file paths (one per line) <code>-s [source]</code> Source directory to process <code>-o [output]</code> In generate mode, checksum file name; In verify mode, existing checksum file to verify against <code>-v</code> Verify mode <code>-r</code> Recursive mode <code>-n</code> Dry run <code>-a</code> Append to existing checksum file <code>-h</code> Display help message","tags":["hpc","batch-processing","slurm"]},{"location":"man/md5checker/#examples","title":"Examples","text":"<p>Checksum all BAM files in current directory <pre><code>sbatch --job-name=md5-bams md5Checker.sh \\\n    -p \"*.bam\" \\\n    -o project_bams\n</code></pre></p> <p>Checksum files recursively in a directory <pre><code>sbatch --job-name=md5-project md5Checker.sh \\\n    -s /data/project/ \\\n    -r -o project_backup\n</code></pre></p> <p>Checksum files from a list <pre><code>sbatch --job-name=md5-list md5Checker.sh \\\n    -f files_to_check.txt \\\n    -o my_files\n</code></pre></p> <p>Checksum specific pattern recursively <pre><code>sbatch --job-name=md5-fastq md5Checker.sh \\\n    -s /data/fastq/ \\\n    -p \"*.fastq.gz\" \\\n    -r -o fastq_files\n</code></pre></p> <p>Dry run to preview <pre><code>sbatch --job-name=md5-preview md5Checker.sh \\\n    -p \"*.bam\" \\\n    -o test -n\n</code></pre></p> <p>Append to existing checksum file <pre><code>sbatch --job-name=md5-append md5Checker.sh \\\n    -p \"*.cram\" \\\n    -o project_bams -a\n</code></pre></p> <p>Verify all files in a checksum file <pre><code>sbatch --job-name=md5-verify md5Checker.sh \\\n    -v \\\n    -o md5sums-project_bams.txt\n</code></pre></p> <p>Verify only specific files against checksum file <pre><code>sbatch --job-name=md5-verify-bams md5Checker.sh \\\n    -v \\\n    -p \"*.bam\" \\\n    -o md5sums-project_bams.txt\n</code></pre></p> <p>Verify files from a list <pre><code>sbatch --job-name=md5-verify-list md5Checker.sh \\\n    -v \\\n    -f files_to_verify.txt\\\n    -o md5sums-project.txt\n</code></pre></p> <p>Dry run verification <pre><code>sbatch --job-name=md5-verify-preview md5Checker.sh \\\n    -v \\\n    -o md5sums-project.txt -n\n</code></pre></p>","tags":["hpc","batch-processing","slurm"]},{"location":"man/s3mover/","title":"Atelier","text":"","tags":["aws","s3","file-transfer","batch-processing","slurm"]},{"location":"man/s3mover/#overview","title":"Overview","text":"<p>Bidirectional transfer support between local and Amazon S3 locations. It supports batch processing of multiple files, recursive directory operations, and includes comprehensive progress reporting suitable for long-running SLURM jobs.</p>","tags":["aws","s3","file-transfer","batch-processing","slurm"]},{"location":"man/s3mover/#usage","title":"Usage","text":"s3Mover.sh<pre><code>sbatch --job-name=[jobName] ~/atelier/bin/s3Mover.sh -s [source] -d [dest] [options]\n</code></pre>","tags":["aws","s3","file-transfer","batch-processing","slurm"]},{"location":"man/s3mover/#options","title":"Options","text":"Option Description <code>-s [source]</code> Source path - can be: S3 URI or Local path <code>-d [dest]</code> Destination path - can be: S3 URI or Local path <code>-f [fileList]</code> Text file containing list of file paths to transfer (one per line) <code>-r</code> Recursive mode - transfer all files in source directory <code>-n</code> Dry run - preview without making changes <code>-c [class]</code> S3 storage class for uploads (default: STANDARD; avail: GLACIER, DEEP_ARCHIVE) <code>-h</code> Display help message","tags":["aws","s3","file-transfer","batch-processing","slurm"]},{"location":"man/s3mover/#examples","title":"Examples","text":"<p>Upload a local file to S3 <pre><code>sbatch --job-name=upload ~/atelier/bin/s3Mover.sh \\\n    -s /data/sample.bam \\\n    -d s3://mybucket/data/\n</code></pre></p> <p>Upload with GLACIER storage class <pre><code>sbatch --job-name=archive ~/atelier/bin/s3Mover.sh \\\n    -s /data/sample.bam \\\n    -d s3://mybucket/archive/ \\\n    -c GLACIER\n</code></pre></p> <p>Upload directory recursively to DEEP_ARCHIVE <pre><code>sbatch --job-name=archive ~/atelier/bin/s3Mover.sh \\\n    -s /data/sample.bam \\\n    -d s3://mybucket/archive/ \\\n    -r -c DEEP_ARCHIVE\n</code></pre></p> <p>Download from S3 to local <pre><code>sbatch --job-name=download ~/atelier/bin/s3Mover.sh \\\n    -s s3://mybucket/data/file.bam \\\n    -d /local/data/\n</code></pre></p> <p>Download S3 directory recursively <pre><code>sbatch --job-name=download-dir ~/atelier/bin/s3Mover.sh \\\n    -s s3://mybucket/data/project/ \\\n    -d /local/data/project/ \\\n    -r\n</code></pre></p> <p>Move files within S3 <pre><code>sbatch --job-name=s3move ~/atelier/bin/s3Mover.sh \\\n    -s s3://mybucket/data/file.bam \\\n    -d s3://mybucket/archive/\n</code></pre></p> <p>Transfer files from a list (mixed local/S3 sources) <pre><code>sbatch --job-name=batch ~/atelier/bin/s3Mover.sh \\\n    -f files_to_transfer.txt \\\n    -d s3://mybucket/archive/\n</code></pre></p> <p>Dry run to preview operations <pre><code>sbatch --job-name=preview ~/atelier/bin/s3Mover.sh \\\n    -s /data/project/ \\\n    -d s3://mybucket/data/ \\\n    -r -c DEEP_ARCHIVE -n\n</code></pre></p>","tags":["aws","s3","file-transfer","batch-processing","slurm"]},{"location":"man/srafastqextractor/","title":"Atelier","text":"","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/srafastqextractor/#overview","title":"Overview","text":"<p>Extract FASTQ files from prefetched SRA data using fasterq-dump. Supports both public and controlled-access (dbGaP) data.</p>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/srafastqextractor/#usage","title":"Usage","text":"sraFastqExtractor.sh<pre><code>sbatch --job-name=[jobName] ~/atelier/bin/sraFastqExtractor.sh -l [accessionList] [options]\n</code></pre>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/srafastqextractor/#options","title":"Options","text":"Option Description <code>-l [accessionList]</code> Text file containing SRA accession IDs (one ID per line). Assumes accessions are in subdirectories with prefetched .sra files <code>-b [baseDir]</code> ase directory containing accession subdirectories (default: current directory) <code>-n [ngcFile]</code> Path to .ngc file for dbGaP controlled-access data <code>-d</code> Dry run <code>-v</code> Verbose output with debug logging <code>-h</code> Display help message","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/srafastqextractor/#examples","title":"Examples","text":"<p>Basic extraction from public data <pre><code>sbatch --job-name=fastq-extract ~/atelier/bin/sraFastqExtractor.sh \\\n    -l accessions.txt\n</code></pre></p> <p>Extract controlled-access data <pre><code>sbatch --job-name=fastq-dbgap ~/atelier/bin/sraFastqExtractor.sh \\\n    -l accessions.txt \\\n    -n ~/prj_1234.ngc\n</code></pre></p> <p>Dry run preview <pre><code>sbatch --job-name=fastq-preview ~/atelier/bin/sraFastqExtractor.sh \\\n    -l accessions.txt \\\n    -d\n</code></pre></p> <p>Verbose/debug mode <pre><code>sbatch --job-name=fastq-verbose ~/atelier/bin/sraFastqExtractor.sh \\\n    -l accessions.txt \\\n    -v\n</code></pre></p>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/sraprefetcher/","title":"Atelier","text":"","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/sraprefetcher/#overview","title":"Overview","text":"<p>Prefetch SRA data from NCBI using the SRA Toolkit. Supports both public and controlled-access (dbGaP) data.</p>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/sraprefetcher/#usage","title":"Usage","text":"sraPrefetcher.sh<pre><code>sbatch --job-name=[jobName] ~/atelier/bin/sraPrefetcher.sh -l [accessionList] [options]\n</code></pre>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/sraprefetcher/#options","title":"Options","text":"Option Description <code>-l [accessionList]</code> Text file containing SRA accession IDs (one ID per line) <code>-o [outputDir]</code> Output directory for prefetched files (default: current directory) <code>-n [ngcFile]</code> Path to .ngc file for dbGaP controlled-access data <code>-m [maxSize]</code> Maximum download size (default: 500G) <code>-r</code> Resume incomplete downloads <code>-d</code> Dry run <code>-v</code> Verbose output with debug logging <code>-h</code> Display help message","tags":["sra","hpc","batch-processing","slurm"]},{"location":"man/sraprefetcher/#examples","title":"Examples","text":"<p>Basic public data prefetch <pre><code>sbatch --job-name=sra-test ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt\n</code></pre></p> <p>Download to specific directory <pre><code>sbatch --job-name=sra-download ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt \\\n    -o /data/sra/\n</code></pre></p> <p>Controlled-access data with dbGaP key <pre><code>sbatch --job-name=sra-dbgap ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt \\\n    -n prj_1234.ngc\n</code></pre></p> <p>Custom size and resume <pre><code>sbatch --job-name=sra-large ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt \\\n    -m 1T \\\n    -r\n</code></pre></p> <p>Verbose/debug mode <pre><code>sbatch --job-name=sra-verbose ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt \\\n    -v\n</code></pre></p> <p>Dry run preview <pre><code>sbatch --job-name=sra-preview ~/atelier/bin/sraPrefetcher.sh \\\n    -l accessions.txt \\\n    -d\n</code></pre></p>","tags":["sra","hpc","batch-processing","slurm"]},{"location":"recipes/user-collaborator/","title":"Atelier","text":"","tags":["aws","s3","file-transfer"]},{"location":"recipes/user-collaborator/#overview","title":"Overview","text":"<p>Below are IAM policy templates based on extent of permissions given to the user on the collaborator's side. Some example use cases would be:</p> <ul> <li>Providing a collaborator with access to exisiting data held in an S3 bucket to support transfer of data from you to them (one-way download)</li> </ul> <p>Use the 'Download only' template</p> <ul> <li>Providing a collaborator with access to an S3 bucket to support transfer of data from them to you as part of an ongoing project (upload and download)</li> </ul> <p>Use the 'Upload/Download' template</p>","tags":["aws","s3","file-transfer"]},{"location":"recipes/user-collaborator/#templates","title":"Templates","text":"Download onlyUpload/Download <p>This template is designed for creating an IAM policy to attach to an AWS new user account that allows specific collaborators access to download files (read-only) from certain S3 buckets.</p> <p>First, copy this JSON script as the template that will be modified.</p> Download only template<pre><code>{ \n  \"Version\": \"2012-10-17\", \n  \"Id\": \"collaborator-username-download\",\n  \"Statement\": [ \n    { \n      \"Sid\": \"AllowListBucketInSpecificPrefixes\", \n      \"Effect\": \"Allow\", \n      \"Action\": [\"s3:ListBucket\"], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket\", \n      \"Condition\": { \n        \"StringLike\": { \n          \"s3:prefix\": [ \n            \"exports/researchA/\", \n            \"exports/researchB/subset/\"\n          ] \n        } \n      } \n    }, \n    { \n      \"Sid\": \"AllowGetObjectsInSpecificPrefixes\", \n      \"Effect\": \"Allow\", \n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\"\n      ], \n      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", \n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\"\n      ] \n    }, \n    { \n      \"Sid\": \"ExplicitDenyWriteOrDelete\", \n      \"Effect\": \"Deny\", \n      \"Action\": [ \n        \"s3:PutObject\", \n        \"s3:DeleteObject\", \n        \"s3:DeleteObjectVersion\", \n        \"s3:AbortMultipartUpload\", \n        \"s3:PutObjectAcl\", \n        \"s3:RestoreObject\" \n      ], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket/exports/\" \n    }\n  ] \n}\n</code></pre> <p>Next, modify the template to use the specific bucket name and prefixes as needed for the collaboration.</p> <p>Line 9<pre><code>      \"Action\": [\"s3:ListBucket\"], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket\", # Change here\n      \"Condition\": { \n</code></pre> Lines 13 &amp; 14<pre><code>      \"s3:prefix\": [ \n        \"exports/researchA/\", # Change / Add Here\n        \"exports/researchB/subset/\" # Change / Add here\n      ] \n</code></pre> Lines 27 &amp; 28<pre><code>      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", # Change / Add here\n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\" # Change / Add here\n      ] \n</code></pre> Line 42<pre><code>      ], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket/exports/\" # Change here\n    }\n</code></pre></p> <p>Last, give the policy a specific ID for this collaboration. The policy will be removed after the collaboration ends or as needed.</p> Line 3<pre><code>{ \n  \"Version\": \"2012-10-17\",\n  \"Id\": \"collaborator-username-download\", # Change here\n  \"Statement\": [\n</code></pre> <p>This template is designed for creating an IAM policy to attach to an AWS new user account that allows specific collaborators access to upload and download files (read-write-delete) from certain S3 buckets.</p> <p>First, copy this JSON script as the template that will be modified.</p> Upload/Download template<pre><code>{ \n  \"Version\": \"2012-10-17\", \n  \"Id\": \"collaborator-username-readwrite\", \n  \"Statement\": [ \n    { \n      \"Sid\": \"AllowListBucketInSpecificPrefixes\", \n      \"Effect\": \"Allow\", \n      \"Action\": [\"s3:ListBucket\"], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket\", \n      \"Condition\": { \n        \"StringLike\": { \n          \"s3:prefix\": [ \n            \"exports/researchA/\",\n            \"exports/researchB/subset/\" \n          ] \n        } \n      } \n    }, \n    { \n      \"Sid\": \"AllowReadWriteObjectsInSpecificPrefixes\", \n      \"Effect\": \"Allow\", \n      \"Action\": [ \n        \"s3:GetObject\", \n        \"s3:GetObjectVersion\", \n        \"s3:PutObject\" \n      ],    \n      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", \n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\" \n      ] \n    }, \n    { \n      \"Sid\": \"OptionalAllowDeleteInSpecificPrefixes\", \n      \"Effect\": \"Allow\", \n      \"Action\": [ \n        \"s3:DeleteObject\", \n        \"s3:DeleteObjectVersion\", \n        \"s3:AbortMultipartUpload\" \n      ], \n      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", \n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\" \n      ] \n    } \n  ] \n}\n</code></pre> <p>Next, modify the template to use the specific bucket name and prefixes as needed for the collaboration.</p> <p>Line 9<pre><code>      \"Action\": [\"s3:ListBucket\"], \n      \"Resource\": \"arn:aws:s3:::my-data-bucket\", # Change here\n      \"Condition\": { \n</code></pre> Lines 13 &amp; 14<pre><code>      \"s3:prefix\": [ \n        \"exports/researchA/\", # Change / Add Here\n        \"exports/researchB/subset/\" # Change / Add here\n      ] \n</code></pre> Lines 28 &amp; 29<pre><code>      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", # Change / Add here\n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\" # Change / Add here\n      ] \n</code></pre> Lines 41 &amp; 42<pre><code>      \"Resource\": [ \n        \"arn:aws:s3:::my-data-bucket/exports/researchA/*\", # Change / Add here\n        \"arn:aws:s3:::my-data-bucket/exports/researchB/subset/*\" # Change / Add here\n      ] \n</code></pre></p> <p>Now the rest of the process is completed within the AWS Console using the steps below.</p> <ul> <li>Create the IAM policy:</li> </ul> <p>IAM console &gt; Policies &gt; Create policy &gt; JSON</p> <p>Paste the JSON above with your values &gt; Create policy.</p> <ul> <li>Create a group and attach the policy:</li> </ul> <p>IAM console &gt; User groups &gt; Create group (e.g., ExternalS3ReadOnly)</p> <p>Attach the S3ReadOnlySpecificPrefixes policy to the group.</p> <ul> <li>Create the IAM user and add to the group:</li> </ul> <p>IAM console &gt; Users &gt; Create user</p> <p>Add the user to ExternalS3ReadOnly</p> <p>Create Access Key with CLI programmatic access</p> <p>Save the Access Key ID and Secret Access Key and share securely.</p>","tags":["aws","s3","file-transfer"]}]}